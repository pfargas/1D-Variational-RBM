{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from torch_wavefunction import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WavefunctionResults:\n",
    "    def __init__(self):\n",
    "        self.wavefunctions = []\n",
    "        self.convergence_energies = []\n",
    "        self.final_wavefunctions = []\n",
    "        self.final_energies = []\n",
    "        self.torch_wavefunctions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_state(results: WavefunctionResults, x_train,layout=[1,60,60,1], **kwargs):\n",
    "    model = WaveFunctionMLP(layout).to(device)\n",
    "    \n",
    "    if \"callback\" in kwargs:\n",
    "        callback_kwargs = kwargs[\"callback\"]\n",
    "        assert isinstance(callback_kwargs, dict), \"callback_kwargs must be a dictionary\"\n",
    "    else:\n",
    "        callback_kwargs = {}\n",
    "    if \"train\" in kwargs:\n",
    "        train_kwargs = kwargs[\"train\"]\n",
    "        assert isinstance(train_kwargs, dict), \"train_kwargs must be a dictionary\"\n",
    "    else:\n",
    "        train_kwargs = {}\n",
    "    \n",
    "    \n",
    "    for key in kwargs:\n",
    "        if ((key != \"callback\") and (key != \"train\")):\n",
    "            print(f\"Unknown keyword argument: {key}\")\n",
    "    \n",
    "    if \"patience\" not in callback_kwargs:\n",
    "        callback_kwargs['patience'] = 250\n",
    "    if \"min_delta\" not in callback_kwargs:\n",
    "        callback_kwargs['min_delta'] = 0.0001\n",
    "        \n",
    "    if \"lr\" not in train_kwargs:\n",
    "        train_kwargs['lr'] = 0.001\n",
    "    if \"epochs\" not in train_kwargs:\n",
    "        train_kwargs['epochs'] = 1000\n",
    "    if \"save_wavefunction_history\" not in train_kwargs:\n",
    "        train_kwargs['save_wavefunction_history'] = False\n",
    "    # if \"save_energy_history\" not in train_kwargs:\n",
    "    #     train_kwargs['save_energy_history'] = False\n",
    "    if \"previous_wavefunctions\" not in train_kwargs:\n",
    "        train_kwargs['previous_wavefunctions'] = None\n",
    "    \n",
    "    early_stopping = EarlyStoppingCallback(patience = callback_kwargs['patience'], min_delta =  callback_kwargs['min_delta'])\n",
    "    psi_normalized_cpu, energy_fin, energy_hist, wf_hist, psi_normalized_torch = train_wavefunction(model, \n",
    "                                                                                                    x_train, \n",
    "                                                                                                    callback=early_stopping, \n",
    "                                                                                                    **train_kwargs)\n",
    "    results.wavefunctions.append(wf_hist)\n",
    "    results.convergence_energies.append(energy_hist)\n",
    "    results.final_wavefunctions.append(psi_normalized_cpu)\n",
    "    results.final_energies.append(energy_fin)\n",
    "    results.torch_wavefunctions.append(psi_normalized_torch)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in the model has to have the shape `[N_batch,N_input]`, where `N_batch` is the batch size (or in our case the number of points in space), and `N_input` is the number of input neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dim = torch.linspace(-10, 10, 10, dtype=torch.float32, device=device, requires_grad=True).view(-1, 1)\n",
    "y_dim = torch.linspace(-10, 10, 10, dtype=torch.float32, device=device, requires_grad=True).view(-1, 1)\n",
    "x_train = torch.stack([x_dim, y_dim], dim=1).to(device)\n",
    "\n",
    "# transpose x_Train\n",
    "x_train = (torch.squeeze(x_train)).detach()\n",
    "x_train.requires_grad = True\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pfargas/software/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m      7\u001b[0m train_kwargs_0 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_wavefunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m     }\n\u001b[1;32m     15\u001b[0m options_0 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m\"\u001b[39m: callback_kwargs_0,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_kwargs_0,\n\u001b[1;32m     18\u001b[0m     }\n\u001b[0;32m---> 20\u001b[0m train_single_state(results, x_train, layout\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m60\u001b[39m,\u001b[38;5;241m60\u001b[39m,\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions_0)\n",
      "Cell \u001b[0;32mIn[3], line 37\u001b[0m, in \u001b[0;36mtrain_single_state\u001b[0;34m(results, x_train, layout, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     train_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprevious_wavefunctions\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     36\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStoppingCallback(patience \u001b[38;5;241m=\u001b[39m callback_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatience\u001b[39m\u001b[38;5;124m'\u001b[39m], min_delta \u001b[38;5;241m=\u001b[39m  callback_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_delta\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 37\u001b[0m psi_normalized_cpu, energy_fin, energy_hist, wf_hist, psi_normalized_torch \u001b[38;5;241m=\u001b[39m train_wavefunction(model, \n\u001b[1;32m     38\u001b[0m                                                                                                 x_train, \n\u001b[1;32m     39\u001b[0m                                                                                                 callback\u001b[38;5;241m=\u001b[39mearly_stopping, \n\u001b[1;32m     40\u001b[0m                                                                                                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_kwargs)\n\u001b[1;32m     41\u001b[0m results\u001b[38;5;241m.\u001b[39mwavefunctions\u001b[38;5;241m.\u001b[39mappend(wf_hist)\n\u001b[1;32m     42\u001b[0m results\u001b[38;5;241m.\u001b[39mconvergence_energies\u001b[38;5;241m.\u001b[39mappend(energy_hist)\n",
      "File \u001b[0;32m~/Desktop/PhD/1D-Variational-RBM/python/torch_wavefunction.py:185\u001b[0m, in \u001b[0;36mtrain_wavefunction\u001b[0;34m(model, x_train, epochs, lr, print_interval, save_wavefunction_history, previous_wavefunctions, overlap_penalty, callback)\u001b[0m\n\u001b[1;32m    182\u001b[0m     overlap_loss\u001b[38;5;241m=\u001b[39moverlap_penalty \u001b[38;5;241m*\u001b[39m orthogonality_loss\n\u001b[1;32m    183\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39moverlap_loss\u001b[38;5;241m.\u001b[39mreal  \u001b[38;5;66;03m# Weight the orthogonality loss\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m loss\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    186\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    188\u001b[0m energy_history\u001b[38;5;241m.\u001b[39mappend(energy\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m.\u001b[39mreal)\n",
      "File \u001b[0;32m~/software/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    650\u001b[0m )\n",
      "File \u001b[0;32m~/software/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:346\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    337\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    338\u001b[0m     (inputs,)\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m    343\u001b[0m )\n\u001b[1;32m    345\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 346\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/software/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:199\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    197\u001b[0m     out_numel_is_1 \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_numel_is_1:\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    201\u001b[0m     )\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_dtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[1;32m    203\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "\n",
    "results = WavefunctionResults()\n",
    "\n",
    "callback_kwargs_0 = {\n",
    "    \"patience\": 250,\n",
    "    \"min_delta\": 1e-5,\n",
    "    }\n",
    "train_kwargs_0 = {\n",
    "    \"lr\": 0.001,\n",
    "    \"epochs\": 1000,\n",
    "    \"save_wavefunction_history\": True,\n",
    "    # \"save_energy_history\": True,\n",
    "    \"previous_wavefunctions\": None,\n",
    "    }\n",
    "\n",
    "options_0 = {\n",
    "    \"callback\": callback_kwargs_0,\n",
    "    \"train\": train_kwargs_0,\n",
    "    }\n",
    "\n",
    "train_single_state(results, x_train, layout=[2,60,60,1], **options_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveFunctionMLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, layer_dims=[1,60,60,1]):\n",
    "        super(WaveFunctionMLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            layers.append(nn.Linear(layer_dims[i], layer_dims[i+1]))\n",
    "            if i < len(layer_dims) - 2:  # Adding activation function for all layers except the last lasyer\n",
    "                layers.append(nn.Tanh())\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WaveFunctionMLP([2,1,1]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([3])\n",
      "torch.Size([2, 10, 1])\n",
      "torch.Size([3, 2])\n",
      "torch.Size([10, 2])\n",
      "tensor([[-0.0302],\n",
      "        [-0.0313],\n",
      "        [-0.0321],\n",
      "        [-0.0328],\n",
      "        [-0.0333],\n",
      "        [-0.0337],\n",
      "        [-0.0340],\n",
      "        [-0.0342],\n",
      "        [-0.0344],\n",
      "        [-0.0345]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_argument_noT = torch.tensor([[1.0, 2.0, 5.0],[3.0, 4.0, 6.0]], device=device)\n",
    "test_argument = test_argument_noT.transpose(0, 1)\n",
    "print(test_argument.shape)\n",
    "\n",
    "first_degree = torch.tensor([1.0,2.0, 3.0], device=device)\n",
    "second_degree = torch.tensor([4.0,5.0, 6.0], device=device)\n",
    "\n",
    "print(first_degree.shape)\n",
    "\n",
    "first_degree = torch.linspace(1, 2, 10, dtype=torch.float32, device=device).view(-1, 1)\n",
    "second_degree = torch.linspace(3, 4, 10, dtype=torch.float32, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "test_arg_combined = torch.stack([first_degree, second_degree], dim=0)\n",
    "\n",
    "print(test_arg_combined.shape)\n",
    "\n",
    "test_arg_combined = (torch.squeeze(test_arg_combined)).transpose(0,1)\n",
    "\n",
    "print(test_argument.shape)\n",
    "print(test_arg_combined.shape)\n",
    "output = model(test_arg_combined)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
